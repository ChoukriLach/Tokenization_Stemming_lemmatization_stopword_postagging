{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea2a8aa",
   "metadata": {},
   "source": [
    "#### In this notebook we will demostrate how to perform tokenization,stemming,lemmatization and pos_tagging using libraries like spacy and nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e4f6db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.5.3-cp310-cp310-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 12.2/12.2 MB 8.8 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     ------------------------------------- 181.6/181.6 kB 11.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy) (22.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.9-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 11.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-win_amd64.whl (94 kB)\n",
      "     ---------------------------------------- 94.7/94.7 kB ? eta 0:00:00\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-win_amd64.whl (480 kB)\n",
      "     ------------------------------------- 480.9/480.9 kB 15.2 MB/s eta 0:00:00\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.9/48.9 kB ? eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-win_amd64.whl (18 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 10.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.9 spacy-3.5.3 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.7.0 wasabi-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install nltk\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57ab518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will be our corpus which we will work on\n",
    "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
    "corpus = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e9132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run 4 times !!\n"
     ]
    }
   ],
   "source": [
    "#lower case the corpus\n",
    "corpus = corpus.lower()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19eb3fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run  times !!\n"
     ]
    }
   ],
   "source": [
    "#removing digits in the corpus\n",
    "import re\n",
    "corpus = re.sub(r'\\d+','', corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "556ccc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook and it should be done soon  it should be done by the ending of this month but will it this notebook has been run  times \n"
     ]
    }
   ],
   "source": [
    "#removing punctuations\n",
    "import string\n",
    "corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "820a964c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need to finalize the demo corpus which will be used for this notebook and it should be done soon it should be done by the ending of this month but will it this notebook has been run times'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing trailing whitespaces\n",
    "corpus = ' '.join([token for token in corpus.split()])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e68cc8",
   "metadata": {},
   "source": [
    "### Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de532c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Choukrallah\n",
      "[nltk_data]     Lachhab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Choukrallah\n",
      "[nltk_data]     Lachhab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "##NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "# By default, the word_tokenize function from NLTK uses the Punkt tokenizer. If you download another tokenizer and want to use it with word_tokenize, you need to explicitly specify the tokenizer you want to use.\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "803eb41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK\n",
      "Tokenized corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'and', 'it', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords: ['need', 'finalize', 'demo', 'corpus', 'used', 'notebook', 'done', 'soon', 'done', 'ending', 'month', 'notebook', 'run', 'times']\n"
     ]
    }
   ],
   "source": [
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_corpus_nltk = word_tokenize(corpus)\n",
    "print(\"\\nNLTK\\nTokenized corpus:\",tokenized_corpus_nltk)\n",
    "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
    "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac0897a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 9.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\choukrallah lachhab\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d921180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spacy:\n",
      "Tokenized Corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'and', 'it', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords ['need', 'finalize', 'demo', 'corpus', 'notebook', 'soon', 'ending', 'month', 'notebook', 'run', 'times']\n"
     ]
    }
   ],
   "source": [
    "##SPACY \n",
    "import spacy\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "stopwords_spacy = spacy_model.Defaults.stop_words\n",
    "print(\"\\nSpacy:\")\n",
    "tokenized_corpus_spacy = word_tokenize(corpus)\n",
    "print(\"Tokenized Corpus:\",tokenized_corpus_spacy)\n",
    "tokens_without_sw= [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
    "\n",
    "print(\"Tokenized corpus without stopwords\",tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23221c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between NLTK and spaCy output:\n",
      " {'used', 'done'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference between NLTK and spaCy output:\\n\",\n",
    "      set(tokenized_corpus_without_stopwords)-set(tokens_without_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecbb37",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03ebfd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stemming:\n",
      "need to finalize the demo corpus which will be used for this notebook and it should be done soon it should be done by the ending of this month but will it this notebook has been run times\n",
      "After Stemming:\n",
      "need to final the demo corpu which will be use for thi notebook and it should be done soon it should be done by the end of thi month but will it thi notebook ha been run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "print(\"Before Stemming:\")\n",
    "print(corpus)\n",
    "\n",
    "print(\"After Stemming:\")\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(stemmer.stem(word),end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a279a16",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08c3cc0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Choukrallah\n",
      "[nltk_data]     Lachhab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Choukrallah\n",
      "[nltk_data]     Lachhab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook and it should be done soon it should be done by the ending of this month but will it this notebook ha been run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(lemmatizer.lemmatize(word),end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036416f",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ec51f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging using spacy:\n",
      "Need : VERB\n",
      "to : PART\n",
      "finalize : VERB\n",
      "the : DET\n",
      "demo : NOUN\n",
      "corpus : NOUN\n",
      "which : PRON\n",
      "will : AUX\n",
      "be : AUX\n",
      "used : VERB\n",
      "for : ADP\n",
      "this : DET\n",
      "notebook : NOUN\n",
      "and : CCONJ\n",
      "it : PRON\n",
      "should : AUX\n",
      "be : AUX\n",
      "done : VERB\n",
      "soon : ADV\n",
      "! : PUNCT\n",
      "! : PUNCT\n",
      ". : PUNCT\n",
      "It : PRON\n",
      "should : AUX\n",
      "be : AUX\n",
      "done : VERB\n",
      "by : ADP\n",
      "the : DET\n",
      "ending : NOUN\n",
      "of : ADP\n",
      "this : DET\n",
      "month : NOUN\n",
      ". : PUNCT\n",
      "But : CCONJ\n",
      "will : AUX\n",
      "it : PRON\n",
      "? : PUNCT\n",
      "This : DET\n",
      "notebook : NOUN\n",
      "has : AUX\n",
      "been : AUX\n",
      "run : VERB\n",
      "4 : NUM\n",
      "times : NOUN\n",
      "! : PUNCT\n",
      "! : PUNCT\n"
     ]
    }
   ],
   "source": [
    "#POS tagging using spacy\n",
    "print(\"POS Tagging using spacy:\")\n",
    "doc = spacy_model(corpus_original)\n",
    "# Token and Tag\n",
    "for token in doc:\n",
    "    print(token,\":\", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "124b53ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Choukrallah\n",
      "[nltk_data]     Lachhab\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging using NLTK:\n",
      "[('Need', 'NN'),\n",
      " ('to', 'TO'),\n",
      " ('finalize', 'VB'),\n",
      " ('the', 'DT'),\n",
      " ('demo', 'NN'),\n",
      " ('corpus', 'NN'),\n",
      " ('which', 'WDT'),\n",
      " ('will', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('used', 'VBN'),\n",
      " ('for', 'IN'),\n",
      " ('this', 'DT'),\n",
      " ('notebook', 'NN'),\n",
      " ('and', 'CC'),\n",
      " ('it', 'PRP'),\n",
      " ('should', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('done', 'VBN'),\n",
      " ('soon', 'RB'),\n",
      " ('!', '.'),\n",
      " ('!', '.'),\n",
      " ('.', '.'),\n",
      " ('It', 'PRP'),\n",
      " ('should', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('done', 'VBN'),\n",
      " ('by', 'IN'),\n",
      " ('the', 'DT'),\n",
      " ('ending', 'VBG'),\n",
      " ('of', 'IN'),\n",
      " ('this', 'DT'),\n",
      " ('month', 'NN'),\n",
      " ('.', '.'),\n",
      " ('But', 'CC'),\n",
      " ('will', 'MD'),\n",
      " ('it', 'PRP'),\n",
      " ('?', '.'),\n",
      " ('This', 'DT'),\n",
      " ('notebook', 'NN'),\n",
      " ('has', 'VBZ'),\n",
      " ('been', 'VBN'),\n",
      " ('run', 'VBN'),\n",
      " ('4', 'CD'),\n",
      " ('times', 'NNS'),\n",
      " ('!', '.'),\n",
      " ('!', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "#pos tagging using nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "print(\"POS Tagging using NLTK:\")\n",
    "pprint(nltk.pos_tag(word_tokenize(corpus_original)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b68509",
   "metadata": {},
   "source": [
    "There are various other libraries you can use to perform these common pre-processing steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
